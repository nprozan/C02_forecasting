---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 2'
geometry: margin=.95in
output: bookdown::pdf_document2
fontsize: 10pt
---

# The Keeling Curve

```{r load packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tsibble)
library(latex2exp)

library(magrittr)
library(patchwork)
library(dplyr)

library(lubridate)
library(Hmisc)

library(feasts)
library(forecast)
library(fable)

library(sandwich)
library(lmtest)

library(fpp3)
library(blsR)

library(ggplotify)

library(RSocrata)
library(MASS)
library(tseries)
library(car)

theme_set(theme_minimal())
knitr::opts_chunk$set(dpi=1000)
```

## (3 points) Task 0a: Introduction

Our study investigates past and future trends of global carbon dioxide (CO2) levels by utilizing historical CO2 data gathered from 1959 to present time in 1997. Specifically, the study aims to not only characterize existing historical trends but also forecast a best estimate of the future course of CO2 levels.

Dating back to Keeling's work in the late 1950s, the implication of CO2 levels on the earth's climate was recognized as a concern and a necessary area for study. With the existing dataset (Jan. 1959 -- Dec. 1997) and predictive methods, the study aims to both understand existing historical trends and provide future estimates in atmospheric CO2 levels. Due to human activities, the future trajectory of CO2 levels can still be influenced (whether exacerbated or muted) making estimation both difficult and critical regarding policy decisions. Additionally, any underlying trends, seasonal oscillations, and irregular elements CO2 levels hoped to be detected.

To achieve this, a linear time trend model will be used to understand the existing historical progression of the CO2 series, and the linear trend will be compared with quadratic and polynomial time trend models to evaluate best performance. To account for seasonal variations, an Autoregressive Integrated Moving Average (ARIMA) model will also be utilized, which will be utilized in forecasting CO2 levels up to 2022.

Additionally, the study will predict when atmospheric CO2 concentrations will reach 420 ppm and 500 ppm, realizing potential environmental impacts at these thresholds. Finally, a forecast for the year 2100 will be performed.

These predictions, while inherently uncertain due to dependence on future human behavior, aim to provide best possible estimates into future atmospheric scenarios. With an overall goal of enhancing current understanding of both prior CO2 trends and its future path, the study can be utilized as critical piece of information for any necessary policy action. Of course, CO2 levels is an evolving situation and refinements to these forecasts are necessary as new data becomes available.

## (3 points) Task 1a: CO2 data

The data is generated by continuously measuring the C02 levels in the atmosphere near Mauna Loa and taking the average every month.

As shown in the plot of the CO2 time series below, CO2 ppm levels exhibit a persistent increasing trend, with seasonal oscillations around the trend. Overall, the CO2 levels are uniformly distributed, with no outliers, and slightly concentrated in lower levels.

```{r co2 eda - time series + dist, fig.width=10, fig.height=5, echo = FALSE, message = FALSE, warning=FALSE}
co2 <- co2 %>%
  as_tsibble()

## Plot series/ACF/PACF/Distribution

co2.plt <- co2%>%
  autoplot(value) +
  labs(y = "ppm", x = "Year Month", title = "CO2 Levels (1959 - 1997)")

co2.dist.plt <- hist(co2$value, breaks = 50, plot = FALSE)

(co2.plt|as.ggplot(~plot(co2.dist.plt, main = "CO2 Level Distribution (1959 - 1997)", xlab  = "CO2 (ppm)")))
```

Examining the ACF and PACF of the CO2 levels below, we see very persistent autocorrelations between period lags, with seasonal oscillations that has periods of about 12 months. Given the persistently high autocorrelations, the original monthly series of CO2 levels is not stationary.

```{r co2 eda - acf + pacf, fig.width=10, fig.height=5, echo = FALSE}
co2.acf <- acf(co2$value, lag.max = 50, plot = FALSE)
co2.pacf <- pacf(co2$value, lag.max = 50, plot = FALSE)

(as.ggplot(~plot(co2.acf, main = "CO2 Level ACF (1959 - 1997)"))|
    as.ggplot(~plot(co2.pacf, main = "CO2 Level PACF (1959 - 1997)")))
```

We would like to further explore the underlying trend and seasonality of CO2 levels. First, we found that annual average CO2 levels exhibit an increasing trend, at an average growth rate of 1.26ppm or 0.37% per annum. Looking at the CO2 y/y increase chart, we observe that the growth of CO2 levels was slightly higher after 1975. We also observed a few large outliers in summer of 1973 and spring of 1988, and small outliers in spring of 1971 and summer of 1974 and 1993. In general, the year over year growth of CO2 level in spring months is larger, evidenced by the box plot below.

To understand seaonal differences, we looked at CO2 levels distributions by month in year. The means of CO2 in each month in year has a sinusoidal shape, with May/June being the high points and Sep/Oct being the low points. The amplitude of the oscillation is roughly 5ppm.

```{r co2 eda - annual + season, fig.width=10, fig.height=5, echo = FALSE}
## Plot Annual/Season Average
co2.annuals <- co2 %>%
  index_by(year = year(index)) %>%
  summarise(avg.co2 = mean(value)) %>%
  mutate(co2.growth = (avg.co2 - lag(avg.co2)),
         co2.growthrate = (avg.co2 - lag(avg.co2))/lag(avg.co2))
co2.annuals.plt <- co2.annuals%>%
  autoplot(avg.co2) +
  labs(y = "Year Avg (ppm)", x = "Year", title = "Annual Average CO2 Levels (1959 - 1997)")
co2.months <- co2 %>%
  mutate(month = month(index, label = TRUE))
co2.months.bx <- co2.months%>%
  ggplot(aes(y = value, x = factor(month))) + 
  geom_boxplot(aes(fill = factor(month))) +
  ggtitle("CO2 by Month in Year") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("CO2 (ppm)") + xlab("Month in Year")
  

co2.annuals.plt|co2.months.bx
```

```{r co2 eda - trend growth, fig.width=10, fig.height=5, echo = FALSE, warning=FALSE}
ppm.annual.growth <- co2.annuals %>% ggplot(
  aes(y = co2.growth, x = year)) + 
  geom_bar(stat = "identity") +
  ylab("CO2 Annual Change (ppm)") + xlab("Year") + 
  ggtitle("Annual Average CO2 Growth (ppm)")

rate.annual.growth <- co2.annuals %>% ggplot(
  aes(y = co2.growthrate, x = year)) + 
  geom_bar(stat = "identity") + 
  ylab("CO2 Annual Change (%)") + xlab("Year") + 
  ggtitle("Annual Average CO2 Growth Rate (%)")

ppm.growth.avg <- round(mean(co2.annuals$co2.growth, na.rm = TRUE),2)
rate.growth.avg <- round(mean(co2.annuals$co2.growthrate, na.rm = TRUE)*100, 2)

#paste("Annual Average CO2 Growth (ppm): ", ppm.growth.avg )
#paste("Annual Average CO2 Growth (%): ", rate.growth.avg )

# ppm.annual.growth|rate.annual.growth
```



```{r co2 eda - y/y changes, fig.width=10, fig.height=5, echo = FALSE, warning=FALSE}
annual.change <- co2 %>% 
  mutate(yy.change = value - lag(value, 12),
         yy.growthrate = yy.change/lag(value, 12)*100)
yy.change.plt <- annual.change %>%
  autoplot(yy.change) +
  labs(y = "y/y change (ppm)", x = "Year Month", title = "CO2 y/y (1959 - 1997)")

yy.change.plt <- annual.change %>%
  autoplot(yy.growthrate) +
  labs(y = "y/y change (%)", x = "Year Month", title = "CO2 y/y (1959 - 1997)")

change.months <- annual.change %>%
  mutate(month = month(index, label = TRUE))
change.months.bx <- change.months%>%
  ggplot(aes(y = yy.change, x = factor(month))) + 
  geom_boxplot(aes(fill = factor(month))) +
  ggtitle("CO2 y/y Change by Month in Year") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("y/y CO2 Change (ppm)") + xlab("Month in Year")
yy.change.plt|change.months.bx

ppm.yy.avg <- round(mean(annual.change$yy.change, na.rm = TRUE),2)
rate.yy.avg <- round(mean(annual.change$yy.growthrate, na.rm = TRUE), 2)

# paste("Average y/y CO2 Growth (ppm): ", ppm.yy.avg )
# paste("Average y/y CO2 Growth (%): ", rate.yy.avg )
```

Using an STL decomposition, a clear upward trend is observed. Seasonal sinusoidal ocsillation appear fairly consistent, with slightly larger amplitude (more downward oscillation) as levels of CO2 increases.

```{r co2 eda - decomp, fig.width=12, fig.height=6, echo = FALSE}
co2.decompose <- co2 %>%
  model(stl = STL(value))
components(co2.decompose) %>%
  autoplot()
```

## (3 points) Task 2a: Linear time trend model

We first fit a linear time trend model. The model gives a coefficient associated with numeric time index that is significant with p-value \<0.001. Hence, we determine that a time-related trend is significant. Although the residuals of the linear trend model appear normally distributed, the time series of the residuals show that 1) there is possible non-linear component of the CO2 levels, evidenced by the obvious curvature of the residuals 2) there are possible seasonal effects that are not being captured by the linear model, evidenced by the obvious regular oscillations of the residuals around the curvature. As a result, we think an exploration of non-linear models such as quadratic and polynomial time trend models is warranted.

```{r linear trend model, fig.width=10, fig.height=5, echo=FALSE, message=FALSE, warning=FALSE}
## Linear trend model
co2$num.index <-  as.numeric(co2$index)
linear.model <- lm(value ~ num.index, data = co2)
#summary(linear.model)
## Residuals
lin.mod.resid.dist <- hist(linear.model$residuals, breaks = 15, plot = FALSE)
lin.mod.resid <- data.frame(index = co2$index, resid = linear.model$residuals) %>%
  as_tsibble()
lin.mod.resid.plt <- lin.mod.resid %>%
  autoplot(resid) + 
  labs(y = "ppm", x = "Year Month", title = "Linear Model Residual")
  
(lin.mod.resid.plt|as.ggplot(~plot(lin.mod.resid.dist, main = "Linear Model Residual Histogram")))
```

After fitting a quadratic model, we found that the second order time term also has a significant coefficient. The residual is also normally distributed, but with less obvious curvature. However, the quadratic model still suffer from not capturing seasonal oscillations, which is visible from the error terms. The decreasing trend of residuals after 1990 could merit more analysis of additional non-linear terms, such as higher polynomial order terms.

```{r quad trend model, fig.width=10, fig.height=5, echo = FALSE, message=FALSE, warning=FALSE}
## Quadratic trend model
quad.model <- lm(value ~ num.index + I(num.index^2), data = co2)
#summary(quad.model)
## Residuals
quad.mod.resid.dist <- hist(linear.model$residuals, breaks = 15, plot = FALSE)
quad.mod.resid <- data.frame(index = co2$index, resid = quad.model$residuals) %>%
  as_tsibble()
quad.mod.resid.plt <- quad.mod.resid %>%
  autoplot(resid) + 
  labs(y = "ppm", x = "Year Month", title = "Quadratic Model Residual")
  
(quad.mod.resid.plt|as.ggplot(~plot(quad.mod.resid.dist, main = "Quadratic Model Residual Histogram")))

```

```{r box cox lambda, echo = FALSE, message=FALSE}
## Log transformation - Box-Cox
bc.lambda <- BoxCox.lambda(co2$value)
```

Before diving into higher order models, we wish to examine if any transformation would be needed on the original series. Using the Box-Cox transformation method, we found the optimal lambda with Box-Cox is around `r signif(bc.lambda, 2)`, suggesting that the optimal transformation is approximately a log-transformation. However, as seen in the EDA above, the data goes from `r min(co2$value)` to `r max(co2$value)` which doesn't span multiple orders of magnitude. Thus, a log transform wouldn't change the date in any significant way. This is shown visually in Appendix 1. As a result, we do not think a log transformation would be necessary.

Below we deploy the polynomial model with seasonal adjustment for CO2 levels. We use a combination of AIC, AICc and BIC to select the degree of polynomial. We conclude that the improvement after degree 3 in AIC/AICc/BIC is limited. Thus an order of larger than 3 may not be appropriate. Splitting the original 1997 dataset into a training and testing set, we found that degree 3 polynomial model has the lowest RMSE for the testing set of 2 years. The performance of each polynomial model can be found in appendix 2.

```{r polynomial model, echo = FALSE, message = FALSE}
## Polynomial + seasonal
co2$month <- month(co2$index)
poly.deg <- seq(1:5)
poly.aic <- c()
poly.aicc <- c()
poly.bic <- c()
for (n in poly.deg){
  poly.mod <- co2 %>%
    model(TSLM(value ~ poly(trend(), n) + season()))
  poly.aic <- c(poly.aic, glance(poly.mod)$AIC)
  poly.aicc <- c(poly.aicc, glance(poly.mod)$AICc)
  poly.bic <- c(poly.bic, glance(poly.mod)$BIC)
}
poly.eval <- data.frame(Deg = poly.deg, AIC = poly.aic, AICc = poly.aicc, BIC = poly.bic)
#poly.eval

test.size <- 12*2
co2.train <- co2 %>%
  slice(1:(n()-test.size))

poly.model <- co2.train %>%
  model(poly1 = TSLM(value ~ poly(trend(), 1, raw = TRUE) + season()),
        poly2 = TSLM(value ~ poly(trend(), 2, raw = TRUE) + season()),
        poly3 = TSLM(value ~ poly(trend(), 3, raw = TRUE) + season()),
        poly4 = TSLM(value ~ poly(trend(), 4, raw = TRUE) + season())) 
poly.forecast <- poly.model %>% forecast(h=24)

#fabletools::accuracy(poly.forecast, co2)

poly.select.model <- co2 %>%
  model(poly2 = TSLM(value ~ poly(trend(), 2, raw = TRUE) + season()),
        poly3 = TSLM(value ~ poly(trend(), 3, raw = TRUE) + season()))
poly.select.forecast <- poly.select.model %>% forecast(h=12*23)

forecast.2020 <- poly.select.forecast %>%
  filter(year(index) == "2020") %>%
  index_by(year = year(index)) %>%
  group_by(.model) %>%
  summarise(mean.forecast = mean(.mean))
```

Using the degree 3 polynomial, the forecasted CO2 level would start to bend, with growth rate decreasing and reaching around average `r round(forecast.2020[2, 3])`ppm in 2020. We think that there is no evidence that the upward trend should slow down at the time of the prediction, and thus decide to explore the second best performing model (degree 2) based on the selection process above. Additionally, the curvature of degree 3 polynomial would make it less appropriate for longer-term forecast. Using the degree 2 polynomial, we obtained a forecast of around `r round(forecast.2020[1, 3])`ppm average in 2020.

```{r polynomial forecast, fig.width=10, fig.height=4, echo = FALSE}
## Forecast w/ Polynomial
co2 %>%
  autoplot(value) +
  geom_line(data = fitted(poly.select.model),
            aes(y = .fitted, colour = .model)) + 
  autolayer(poly.select.forecast, alpha = 0.7, level = 95) +
  ggtitle("Polynomial models forecast comparison")
```

## (3 points) Task 3a: ARIMA times series model

From the ACF/PACF of the original CO2 levels time series (see EDA), we could conclude that the original series is not stationary, given the persistently higher autocorrelations (Non-stationarity is confirmed by both ADF and Unit root tests). From the prior EDA process, we think a seasonal component should be modeled. Hence, we think a double difference (12-month seasonal differencing + regular differencing) is necessary to obtain a stationary time series. As shown in the graphs below, we found that double differenced CO2 levels series appear stationary, with no persistent ACF. From ACF/PACF, we see lag 1 and lag 3 having significant (partial) autocorrelations, along with significant (partial) autocorrelations at lags of multiples of 12, which we attribute to seasonal lags being significant. We confirmed the stationarity of the double-differenced series via both ADF and KPSS Unit root tests (see Appendix 3). With the aid of grid searching parameters, with narrowed down our selection of best ARIMA models to the following three: ARIMA(0,1,3)(1,1,2)[12] based on AIC/AICc metrics with specified search range, as well as default exhaustive search; ARIMA(0,1,1)(1,1,2)[12] based on BIC with specified search range; ARIMA(1,1,1)(1,1,2)[12] following default stepwise procedure.

```{r ARIMA model specification, fig.width=10, fig.height=5, echo = FALSE, message=FALSE, warning=FALSE}
## Explore specification
co2$double.diff <- difference(difference(co2$value, lag = 12))
dd.acf <- acf(co2$double.diff[14:nrow(co2)], lag.max = 50, plot = FALSE)
dd.pacf <- pacf(co2$double.diff[14:nrow(co2)], lag.max = 50, plot = FALSE)

dd.plt <- co2 %>%
  autoplot(double.diff) +
  ggtitle("Double-Diff CO2 Level")
dd.dist <- hist(co2$double.diff, breaks = 15, plot = FALSE)

dd.plt|as.ggplot(~plot(dd.dist, main = "Double-Diff CO2 Level Histogram"))

(as.ggplot(~plot(dd.acf, main = "Double-Diff CO2 Level ACF (1959 - 1997)"))|
    as.ggplot(~plot(dd.pacf, main = "Double-Diff CO2 Level PACF (1959 - 1997)"))) 
```

```{r ARIMA model specification ctd,echo=FALSE, message=FALSE}
model.aic<- co2 %>%
  model(ARIMA(value ~ 0:1 + pdq(0:5,1:2,0:5) + PDQ(0:5,1:2,0:5), ic="aic", stepwise=F, greedy=F))
#model.aic %>%
  #report()
##ARIMA(0,1,3)(1,1,2)[12] 

model.aicc<- co2 %>%
  model(ARIMA(value ~ 0:1 + pdq(0:5,1:2,0:5) + PDQ(0:5,1:2,0:5), ic="aicc", stepwise=F, greedy=F))
#model.aicc %>%
  #report()
##ARIMA(0,1,3)(1,1,2)[12] 

model.bic<- co2 %>%
  model(ARIMA(value ~ 0:1 + pdq(0:5,1:2,0:5) + PDQ(0:5,0:2,0:5), ic="bic", stepwise=F, greedy=F))
#model.bic %>%
  #report()
##ARIMA(0,1,1)(1,1,2)[12] 

model.step<- co2 %>%
  model(ARIMA(value))
#model.step %>%
  #report()
##ARIMA(1,1,1)(1,1,2)[12] 

model.search<- co2 %>%
  model(ARIMA(value, stepwise=FALSE))
#model.search %>%
  #report()
##ARIMA(0,1,3)(1,1,2)[12] 
```

Below table compares the model performance metrics from the selected 3 ARIMA models. We found very similar AIC/AICc metrics across the 3 models, whereas more variation regarding BIC. We further narrowed down our selection to ARIMA(0,1,3)(1,1,2)[12] and ARIMA(0,1,1)(1,1,2)[12] given the underperformance of ARIMA(1,1,1)(1,1,2)[12] based on the three evaluation metrics.

```{r ARIMA model selection, echo= FALSE}
co2.fit <- co2 %>%
  model(arima011112 = ARIMA(value ~ 0 + pdq(0,1,1) + PDQ(1,1,2)),
        arima013112 = ARIMA(value ~ 0 + pdq(0,1,3) + PDQ(1,1,2)),
        arima111112 = ARIMA(value ~ 0 + pdq(1,1,1) + PDQ(1,1,2)))

glance(co2.fit) %>%
  dplyr::select(.model, AIC, AICc, BIC) %>%
  knitr::kable()

arima011112.model <- model.bic
arima013112.model <- model.aic
arima111112.model <- model.step
```

Consequently, we carried out a closer examination of the 3 models to understand their performances. Residuals of both ARIMA models appear to be white noise, with approximately normally distributed residuals. The ACF/PACF shows that mostly no lags are significant, resembling white noise.

```{r evaluate ARIMA models - resid plot, fig.width=10, fig.height=10, echo =FALSE, message=FALSE, warning = FALSE}
arima011112.resid<-arima011112.model %>%
  augment() %>%
  dplyr::select(.resid) %>%
  as.ts()
arima011112.resid.plt <- arima011112.resid%>%autoplot +
  ggtitle("ARIMA(0,1,1)(1,1,2)[12] Residual")
arima011112.resid.dist <- hist(arima011112.resid, plot = FALSE)
arima011112.acf <- acf(arima011112.resid, lag.max = 50, plot = FALSE)
arima011112.pacf <- pacf(arima011112.resid, lag.max = 50, plot = FALSE)

arima013112.resid<-arima013112.model %>%
  augment() %>%
  dplyr::select(.resid) %>%
  as.ts()
arima013112.resid.plt <- arima013112.resid%>%autoplot +
  ggtitle("ARIMA(0,1,3)(1,1,2)[12] Residual")
arima013112.resid.dist <- hist(arima013112.resid, plot = FALSE)
arima013112.acf <- acf(arima013112.resid, lag.max = 50, plot = FALSE)
arima013112.pacf <- pacf(arima013112.resid, lag.max = 50, plot = FALSE)

(arima011112.resid.plt|arima013112.resid.plt)/
  (as.ggplot(~plot(arima011112.resid.dist, main = "ARIMA(0,1,1)(1,1,2)[12] Residual"))|
    as.ggplot(~plot(arima013112.resid.dist, main = "ARIMA(0,1,3)(1,1,2)[12] Residual"))) /
  (as.ggplot(~plot(arima011112.acf, main = "ARIMA(0,1,1)(1,1,2)[12] Residual ACF"))|
    as.ggplot(~plot(arima013112.acf, main = "ARIMA(0,1,3)(1,1,2)[12] Residual ACF"))) /
  (as.ggplot(~plot(arima011112.pacf, main = "ARIMA(0,1,1)(1,1,2)[12] Residual PACF"))|
    as.ggplot(~plot(arima013112.pacf, main = "ARIMA(0,1,3)(1,1,2)[12] Residual PACF"))) 

```

Results from the Box-Ljung tests show that both models test results rejected the null hypothesis of randomly distributed residuals, with ARIMA(0,1,3)(1,1,2)[12] (p = .85) model p-value meaningfully higher than ARIMA(0,1,1)(1,1,2)[12] (p = .25).

```{r evaluate ARIMA models - LjungBox, echo=FALSE, message=FALSE, warning=FALSE}
#Box.test(arima011112.resid, lag = 12, fitdf = 0, type = "Ljung-Box")
#Box.test(arima013112.resid, lag = 12, fitdf = 0, type = "Ljung-Box")
```

To further study the forecast capabilities of the two models, we split the original CO2 levels series to the same training dataset and testing dataset as the polynomial analysis (2yrs testing data). We obtained very similar RMSE for test set, with ARIMA(0,1,3)(1,1,2)[12] performing modestly better. Hence, combined with the Box-Ljung results, we determined ARIMA(0,1,3)(1,1,2)[12] to be our best ARIMA model. See Appendix 4 for details performance statistics.

```{r evaluate ARIMA models - train/test, fig.width=10, fig.height=5, echo =FALSE, message=FALSE, warning = FALSE}
model.comp <- co2.train %>%
  model(arima011112 = ARIMA(value ~ 0 + pdq(0,1,1) + PDQ(1,1,2)),
        arima013112 = ARIMA(value ~ 0 + pdq(0,1,3) + PDQ(1,1,2)))

model.forecast <- forecast(model.comp, h = test.size)
model.forecast %>%
  autoplot(colour="cornflowerblue") +
  autolayer(co2.train, colour="black") +
  geom_line(data=model.comp %>% augment(), aes(index,.fitted,color=.model)) +
  facet_wrap(~.model, ncol=2, nrow=2)
```

```{r ARIMA forecast 2022, echo =FALSE, message=FALSE, warning = FALSE}
arima.fitted <- co2 %>%
  model(arima011112 = ARIMA(value ~ 0 + pdq(0,1,1) + PDQ(1,1,2)),
        arima013112 = ARIMA(value ~ 0 + pdq(0,1,3) + PDQ(1,1,2)))

arima.forecast <- arima.fitted%>%forecast(h = 12*25)

arima.forecast.2022 = arima.forecast %>%
  filter(year(index) == "2022") %>%
  index_by(year = year(index)) %>%
  group_by(.model) %>%
  summarise(mean.forecast = mean(.mean))
```

Applying both models, we generated forecasts to year 2022. The forecasts by the two models are extremely similar, with our selected model ARIMA(0,1,3)(1,1,2)[12] having modestly narrower 95% confidence intervals. ARIMA(0,1,3)(1,1,2)[12] forecasts around `r round(arima.forecast.2022[2, 3])`ppm average for the year 2022.

```{r ARIMA forecast chart,echo =FALSE, message=FALSE, warning = FALSE}
co2 %>%
  autoplot(value) +
  geom_line(data = fitted(arima.fitted),
            aes(y = .fitted, colour = .model)) + 
  autolayer(arima.forecast, alpha = 0.7, level = 95) 
```

## (3 points) Task 4a: Forecast atmospheric CO2 growth

```{r forecast co2 growth, echo = FALSE, message = FALSE, warning=FALSE}
models.partA <- co2 %>%
  model(linear = TSLM(value ~ trend()),
        poly2 = TSLM(value ~ poly(trend(), 2, raw = TRUE) + season()),
        arima013112 = ARIMA(value ~ 0 + pdq(0,1,3) + PDQ(1,1,2)))
models.forecast.partA <- models.partA %>% forecast(h=120*12)
models.forecast.partA.lb <- hilo(models.forecast.partA$value)$lower
models.forecast.partA.ub <- hilo(models.forecast.partA$value)$upper
models.forecast.partA$lb.95 <- models.forecast.partA.lb
models.forecast.partA$ub.95 <- models.forecast.partA.ub


co2 %>%
  autoplot(value) +
  geom_line(data = fitted(models.partA),
            aes(y = .fitted, colour = .model)) +
  autolayer(models.forecast.partA, alpha = 0.7, level = 95) +
  ggtitle("CO2 Forecast")
  
if (isFALSE(getOption('knitr.in.progress'))) {
  ## 420ppm point est
  linear.first.420 <- models.forecast.partA %>%
    filter(.model == "linear") %>%
    filter(.mean >= 420) %>%
    head(1)
  
  poly.first.420 <- models.forecast.partA %>%
    filter(.model == "poly2") %>%
    filter(.mean >= 420) %>%
    head(1)
  poly.last.420 <- models.forecast.partA %>%
    filter(.model == "poly2") %>%
    filter(.mean <= 420) %>%
    tail(1)
  arima.first.420 <- models.forecast.partA %>%
    filter(.model == "arima013112") %>%
    filter(.mean >= 420) %>%
    head(1)
  arima.last.420 <- models.forecast.partA %>%
    filter(.model == "arima013112") %>%
    filter(.mean <= 420) %>%
    tail(1)
  
  
  data.frame(rbind(linear.first.420, poly.first.420, poly.last.420,
                   arima.first.420, arima.last.420))%>%
    dplyr::select(.model, index, .mean)
  
  interval_est = function(model, ppm) {
    first.lb <- models.forecast.partA %>%
      filter(.model == model) %>%
      filter(lb.95 >= ppm) %>%
      head(1)
    first.ub <- models.forecast.partA %>%
      filter(.model == model) %>%
      filter(ub.95 >= ppm) %>%
      head(1)
    last.lb <- models.forecast.partA %>%
      filter(.model == model) %>%
      filter(lb.95 <= ppm) %>%
      tail(1)
    last.ub <- models.forecast.partA %>%
      filter(.model == model) %>%
      filter(ub.95 <= ppm) %>%
      tail(1)
    return(rbind(first.lb, first.ub, last.lb, last.ub))
  }
  
  ## 420ppm interval est
  linear.420.ci = interval_est("linear", 420)
  poly.420.ci = interval_est("poly2", 420)
  arima.420.ci = interval_est("arima013112", 420)
  
  data.frame(rbind(linear.420.ci,
                   poly.420.ci,
                   arima.420.ci)) %>%
    dplyr::select(.model, index, lb.95, ub.95)
  
  ## 500ppm point est
  linear.first.500 <- models.forecast.partA %>%
    filter(.model == "linear") %>%
    filter(.mean >= 500) %>%
    head(1)
  
  poly.first.500 <- models.forecast.partA %>%
    filter(.model == "poly2") %>%
    filter(.mean >= 500) %>%
    head(1)
  poly.last.500 <- models.forecast.partA %>%
    filter(.model == "poly2") %>%
    filter(.mean <= 500) %>%
    tail(1)
  arima.first.500 <- models.forecast.partA %>%
    filter(.model == "arima013112") %>%
    filter(.mean >= 500) %>%
    head(1)
  arima.last.500 <- models.forecast.partA %>%
    filter(.model == "arima013112") %>%
    filter(.mean <= 500) %>%
    tail(1)
  
  data.frame(rbind(linear.first.500, poly.first.500, poly.last.500,
                   arima.first.500, arima.last.500))%>%
    dplyr::select(.model, index, .mean)
  
  ## 500ppm ci est
  linear.500.ci = interval_est("linear", 500)
  poly.500.ci = interval_est("poly2", 500)
  arima.500.ci = interval_est("arima013112", 500)
  
  data.frame(rbind(linear.500.ci,
                   poly.500.ci,
                   arima.500.ci)) %>%
    dplyr::select(.model, index, lb.95, ub.95)
  
  models.forecast.partA %>%
    filter(year(index) == "2100") %>%
    index_by(year = year(index)) %>%
    group_by(.model) %>%
    summarise(mean.forecast = mean(.mean))
}
```


Here are the forecasts associated with each model. The 95% confidence intervals are given in parentheses after each value. The linear model predicts that 420ppm will be crossed around Dec 2041 (Dec 2037, Jan 2046). The polynomial model predicts that 420ppm will happen for the first time in May 2022 (Mar 2022, Apr 2023) and for the last time in Nov 2023 (Oct 2023, Nov 2024). Finally, the ARIMA model predicts that 420ppm will happen for the first time in Apr 2032 (Apr 2022, May 2084) and for the last time in Oct 2035 (Oct 2024, Nov 2117).

The linear model predicts that 500ppm will be crossed around Feb 2103 (Dec 2037, Jan 2046). The polynomial model predicts that 500ppm will happen for the first time in Apr 2051 (Mar 2022, Apr 2023) and for the last time in Oct 2052 (Oct 2023, Nov 2024). Finally, the ARIMA model predicts that 500ppm will happen for the first time May 2084 and for the last time in Oct 2087. As shown in the chart, the ARIMA model predicts that the $CO_2$ levels may never reach 500ppm which prevents us from giving a confidence interval on those values.

We observed that the ARIMA model's confidence interval explodes quickly as the errors accumulate over time. As a result, the predictions are much less reliable than for the trend models. The ARMIA model shouldn't be trusting for long-term forecasts and the trend models should be used instead.

Predicted mean $CO_2$ levels in 2100 by the linear, polynomial and ARIMA models are 497ppm, 685ppm, and 522ppm respectively. Since the ARIMA model has really wide confidence intervals when the forecast horizon is large, we expect the mean $CO_2$ levels from the trend models, more specifically the quadratic model to be more accurate. However, it is generally unadvisable to extrapolate time series over such a long time horizon. For practical reasons, we think that changes in human society could break the current trend of $CO_2$ emissions between now and 2100.


# Report from the Point of View of the Present

## (1 point) Task 0b: Introduction

In a necessary refinement to prior efforts of $CO_2$ trending and future projections performed in 1997, the following study aims to incorporate new higher frequency data and understand the accuracy of the predictions given by the prior model compared to current data. Human behavior has seen progressive shifts over the prior decades which will have certainly affected $CO_2$ trends---in fashion that may have not been perfectly estimated by the 1997 model.

Differing from the 1997 model (which utilized monthly data), the current study will incorporate a data pipeline from the National Oceanic and Atmospheric Administration (NOAA). With this, the study will utilize weekly data which will provide more granular insights. Specifically, the higher frequency data will provide understanding of any additional seasonal variation and allow for more nuanced trending.

After performing an EDA on the new dataset, the projections from the linear and ARIMA models estimated in 1997 will be compared to how $CO_2$ levels have evolved to date. The additional information provided with the new weekly data provide an opportunity for the models to be retrained with both seasonally adjusted and non-seasonally adjusted time series. With this, the future projections with greater robustness will be provided.

Overall, the study will understand the performance of our prior model and provide necessary refinement with higher frequency data and the inclusion of new data from 1997 to date. The refined models will allow us to perform a best estimate of $CO_2$ levels in the year 2122 and the degree of confidence associated with it.


## (3 points) Task 1b: Create a modern data pipeline for Mona Loa CO2 data.

We start off by showcasing the first row of the co2_present dataframe to showcase our completion of the data pull from. In the data set, we had to replace 4 null values. The null values were replaced with the average of the prior month and the following month in order to preserve trend. 

```{r data pipeline, echo=FALSE, message=FALSE}
# co2_present_raw <- read.csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv", comment.char="#") %>% filter(year > 1996) 
co2_present_raw <- read.csv("../data/co2_weekly_mlo.csv", comment.char="#") %>% filter(year > 1996)

#which(co2_present_raw[, "average"] == -999.99)
co2_present_raw[600:602, "average"] <- (co2_present_raw[599, "average"] + co2_present_raw[603, "average"])/2
co2_present_raw[459, "average"] <- (co2_present_raw[458, "average"] + co2_present_raw[460, "average"])/2

co2_present <- co2_present_raw %>%
   mutate(date = make_date(year, month, day)) %>%
       mutate(index = yearmonth(date))

co2_present <- co2_present %>% group_by(index, month, year) %>%
  summarise(average= mean(average)) %>% as_tsibble(index = index)

co2_present.monthly <- data.frame(index = co2_present$index, value = co2_present$average) %>%
  as_tsibble()
knitr::kable(head(co2_present,n= 1))
```

In the charts below, we can see that the general trend of C02 PPM growth is consistent. However, the chart on the right demonstrates that the increase is higher in the recent 20 years than it was before. The red line indicates the average PPM growth in the atmosphere from 1997-today. The blue line, significantly lower, is the PPM average growth rate from prior to 1997. The average rate of PPM growth is 0.56% post 1997, while it was 0.37% prior to 1997. 

```{r 1b EDA, echo = FALSE, warning=FALSE, center = TRUE, fig.align = 'center'}

co2.annuals.plt <-ggplot(co2_present, aes(x = as.Date(paste(year, month, "01", sep = "-")), y = average)) +
  geom_line()+
  labs(y = "Year Avg (ppm)", x = "Year", title = "CO2 Levels (1997 - Today)")


co2.months <- co2 %>%
  mutate(month = month(index, label = TRUE))

annual.change <- co2_present.monthly %>% 
  mutate(yy.change = value - lag(value, 12),
         yy.growthrate = yy.change/lag(value, 12)*100)

ppm.yy.avg2 <- round(mean(annual.change$yy.change, na.rm = TRUE),2)
rate.yy.avg2 <- round(mean(annual.change$yy.growthrate, na.rm = TRUE), 2)

yy.change.plt <- annual.change %>%
  autoplot(yy.change) +
  labs(y = "y/y change (ppm)", x = "Year Month", title = "CO2 y/y (1997-2023)") +
  geom_hline(yintercept = ppm.yy.avg2, linetype = "dashed", color = "red")+ 
  geom_hline(yintercept = ppm.yy.avg, linetype = "dashed", color = "blue") + 
   scale_x_yearmonth(date_labels = "%Y")
  
co2.annuals.plt|yy.change.plt


#print(paste("Average y/y PPM Growth (%): ", ppm.yy.avg2 ))
#print(paste("Average y/y CO2 Growth (%): ", rate.yy.avg2 ))
```
The chart below indicates that there is still seasonality in our PPM levels. Similarly, the spring  months (April,May) are to the highest like they were prior to 1997. The late fall months (September, October) are the lowest levels of PPM, which is consistent with earlier data.

```{r modern data months plot, echo = FALSE, warning=FALSE,fig.width=10, fig.height=4 }
co2.months.bx <- co2.months%>%
  ggplot(aes(y = value, x = factor(month))) + 
  geom_boxplot(aes(fill = factor(month))) +
  ggtitle("CO2 by Month in Year") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("CO2 (ppm)") + xlab("Month in Year")

co2.months.bx
```


## (1 point) Task 2b: Compare linear model forecasts against realized CO2

As we can see in the plot below, the linear model underestimates the CO2 levels compared to the actual data. This could be attributed to rate of change increase mentioned in the earlier section. The 2nd degree polynomial model generates a forecast that best aligns with the realization. This may suggest that CO2 levels follow a polynomial trend.

```{r modeled vs. forecast, echo = FALSE, message = FALSE,  fig.width=10, fig.height=4}
lin.arima.mod.forecast <- models.partA %>% 
  forecast(h=26*12-5) 

lin <- lin.arima.mod.forecast  %>% filter(.model != "arima013112")


co2_present.monthly %>%
  autoplot(value) +
  autolayer(lin, alpha = 0.6, level = 95)   + 
  ggtitle("Pre-1997 Fitted linear/poly. models into Post-1997 data")
```

## (1 point) Task 3b: Compare ARIMA models forecasts against realized CO2

Like the linear model, the AIMA model underestimated the present data. The ARIMA forecast also has a wider prediction interval. Visually, the realized CO2 levels mostly lies within/on the edges of the 95% CI of ARIMA forecasts. The 95% CI of the linear model fails to cover the realized CO2 as we get closer to the present day.

```{r modeled vs. forecast ARIMA, echo = FALSE, message = FALSE, fig.width=10, fig.height=4}
arima.mod.forecast <- models.partA %>% 
  forecast(h=26*12-5)  %>% filter(.model == 'arima013112' )

co2_present.monthly %>%
  autoplot(value) +
  autolayer(arima.mod.forecast, alpha = 0.6, level = 95) + 
  ggtitle("Pre-1997 Fitted ARIMA model into Post-1997 data")

```

## (3 points) Task 4b: Evaluate the performance of 1997 linear and ARIMA models

We can see the model forecasts in the graph below. The black dashed line indicates the time period of 420.

Our 3 model types predict the following dates to reach the 420 threshold. The actual date was 2022 April.

Polynomial: The point estimate reaches 420 in May of 2022. The first time the upper confidence reaches the threshold is in March of 2022, and the last time 420 is included in the 95% CI is in April of 2023.

Polynomial was only one month off! The other models point estimates were decades off.

ARIMA: The point estimate reaches 420 in April of 2032. However, the first time the model reaches 420 in the 95% CI is in April of 2022, which is a solid estimate! Due to the width of the ARIMA model CI, we cannot pinpoint a time that the estimate is greather than 420 with 95% certainty. Thus, ARIMA models are better for short term predictions where the confidence interval is smaller.

Linear: The point estimate crosses 420 in December of 2041. However, the first time the model contains 420 in the 95% CI is in December of 2037, and the time the model predicts better than 420 with 95% certainty is in January of 2046. The slow incline, the lack of seasonality (generally peaks should be predicted in May or April), and a plethora of other reasons indicate to us that this model has low bias.

Looking at the RMSE of the three models below, we can see the clear order of model performance was 1) Polynomial 2) ARIMA and 3) Linear.



```{r evaluate 1997 models, echo = FALSE, message = FALSE, fig.width=10, fig.height=4, warning=FALSE}

lin.arima.mod.forecast <- models.partA %>% 
  forecast(h=600) 

co2_present.monthly %>%
  autoplot(value) +
  autolayer(lin.arima.mod.forecast, alpha = 0.6, level = 95)   + 
  ggtitle("PPM Forecast of the 3 models in A") +
  geom_hline(yintercept = 420, linetype = "dashed", color = "black")



hilo <- hilo(lin.arima.mod.forecast, level = 95)
lin.arima.mod.forecast['lower'] <- as.numeric(sub("\\[(.*),.*\\]95", "\\1", hilo$`95%`))
lin.arima.mod.forecast['upper']  <- as.numeric(sub("\\[.*,(.*)\\]95", "\\1", hilo$`95%`))

arima.mod.forecast.420 <- lin.arima.mod.forecast %>% filter(.model == 'arima013112' ) %>% filter(.mean >= 420)

#arima.mod.forecast.420[1,] 

linear.mod.forecast.420 <- models.partA %>% 
  forecast(h=600)  %>% filter(.model == 'linear' ) %>% filter(.mean >= 420)

#a4 <- lin.arima.mod.forecast %>% filter(.model == 'arima013112' ) %>% filter(upper >= 420)
#linear.mod.forecast.420[1,]

#p4 <- lin.arima.mod.forecast %>%  filter(.model == 'poly2' ) %>% filter(upper >= 420)

#l4 <- lin.arima.mod.forecast %>% filter(.model == 'linear' ) %>% filter(upper >= 420)



#a4 <- lin.arima.mod.forecast %>% filter(.model == 'arima013112' ) %>% filter(lower >= 420)
#linear.mod.forecast.420[1,]

#p4 <- lin.arima.mod.forecast %>%  filter(.model == 'poly2' ) %>% filter(lower >= 420)

#l4 <- lin.arima.mod.forecast %>% filter(.model == 'linear' ) %>% filter(lower >= 420)

poly.mod.forecast.420 <- models.partA %>% 
  forecast(h=26*12-5)  %>% filter(.model == 'poly2' ) %>% filter(.mean >= 420)


knitr::kable(fabletools::accuracy(lin.arima.mod.forecast, co2_present.monthly))
```


## (4 points) Task 5b: Train best models on present data

We start with a view of the seasonal vs. non-seasonal data. We can see below that the Non-Seasonally Adjusted PPM has seasonality, and there tends to be a wave like tendency of the graph. The Seasonally Adjusted component follows the overall trend of the data, but the amplitude of the curves is marginal, or even non-existent.

```{r data for 5b, echo = FALSE, message = FALSE, fig.width=10, fig.height=5, center=TRUE, fig.align='center'}
#NSA data for co2
co2_present_train <- co2_present %>% filter(year(index) < 2021)
co2_present_test <- co2_present %>% filter(year(index) >= 2021)

#decomposing math
co2_present.decompose <- co2_present %>%
  model(stl = STL(average))
co2_present.components <- components(co2_present.decompose) %>% as_tsibble()


co2_present_trainSA <- co2_present.components  %>% filter(year(index) < 2021)
co2_present_trainSA <-  co2_present_trainSA[, c("index", "season_adjust")]
co2_present_testSA <- co2_present.components  %>% filter(year(index) >=  2021)
co2_present_testSA <-  co2_present_testSA[, c("index", "season_adjust")]


merged_data <- merge(co2_present_trainSA, co2_present_train , by = "index", all = TRUE)
# Create a line chart
ggplot(merged_data, aes(x = index),width = 2, height = 1) +
  geom_line(aes(y = season_adjust, color = "SA")) +
  geom_line(aes(y = average, color = "NSA")) +
  labs(x = "Index", y = "Value") +
  scale_color_manual(values = c("SA" = "red", "NSA" = "blue")) +
  ggtitle("Train data for Seasonal and Non seasonal Adjusted PPM")
```


Looking at our Non-Seasonally-Adjusted data, specifically the plot "Single Difference Plot and ACF, PACF (lag = 12)", we notice that there are a number of points on the ACF that are significant, which indicate the plot may  may not be stationary. There are some elements in the graph that have stationary elements. Thus, we take a KPSS test to confirm our findings. The KPSS test returns a value of 0.1, indicating that we can't reject the null hypothesis that the time series is stationary.

Due to our hesitancy, we looked at the first difference and the second difference of the time series. The second difference of the data looks more stationary as can be seen in the plots below. Thus, we chose to model the second difference of the data in the next steps.

```{r trial pipeline, echo = FALSE, message = FALSE, warning = FALSE, out.width="50%", fig.align='center'}
co2_present_train |>
  gg_tsdisplay((difference(average, lag = 12) ), plot_type='partial', lag_max = 36)+ 
  ggtitle("Single Difference Plot and ACF, PACF (lag = 12)")

#co2_present_train |>
  #features(difference(average), unitroot_kpss)

co2_present_train |>
  gg_tsdisplay(difference(difference(average, lag = 12) ), plot_type='partial', lag_max = 36) + 
  ggtitle("Second Difference Plot and ACF,PACF")
```

We run a model "Arima1" which is our first model, with the paramaters ARIMA(1,1,2)(1,1,2)[12]. Our second model "Arima2", has the parameters ARIMA(2,2,1)(2,2,3)[12]. Both models were obtained using stepwise search. When comparing the models to their residual plots, we see that both models appear to have constant variance across the residuals, a roughly normal distribution of errors, and very few significant lags. The model ljung-box test for both confirm that the errors for both models are essentially random (Arima1 p = 0.70, Arima2 p = 0.61).

```{r continue 5b, echo = FALSE, message = FALSE, warning = FALSE, out.width="50%", fig.align='center'}
NSA_models <- co2_present_train %>%  
  model(arima2 = ARIMA(average ~ 0 + pdq(2,2,1) + PDQ(2,2,3)),
        arima1 = ARIMA(average ~ 0 + pdq(1,1,2) + PDQ(1,1,2))
        )

#glance(NSA_models)

arimaTrial <- co2_present_train |>
  model(ARIMA(average ~ 0 + pdq(1,1,2) + PDQ(1,1,2)))

arimaTrial |>
  gg_tsresiduals() +
  ggtitle("ARIMA(1,1,2)(1,1,2)[12] (Arima1) Resid")

#augment(NSA_models) |>
  #filter(.model=='arima1') |>
  #features(.innov, ljung_box, lag = 24, dof = 6)

arimaTrial2  <- co2_present_train |>
  model(ARIMA(average ~ 0 + pdq(2,2,1) + PDQ(2,2,3)))

#report(arimaTrial2)

arimaTrial2 |>
  gg_tsresiduals() + 
  ggtitle("ARIMA(2,2,1)(2,2,3)[12] Resid")
  

#augment(NSA_models) |>
  #filter(.model=='arima2') |>
  #features(.innov, ljung_box, lag = 24, dof = 8)

```

The graph below shows our models and how they fit on a) the train data and b) the test data. At first glance, it is hard to differentiate the models on the train data as the models appear to capture the trend well. In the test data, the Arima1 model appears to be fitting the model better and also has a more narrow confidence interval.

Looking at RMSE in the table below we can confirm that the Arima1 model out performs the Arima2 model on the test data. This will be our model for part 6b.

```{r 5b NSA, echo = FALSE, message = FALSE,warning=FALSE, fig.width=10, fig.height=5, fig.align='center'}
fitted_NSA <- NSA_models %>% fitted()

trainNSA <- co2_present_train %>%
  autoplot(average) +
  autolayer(fitted_NSA, alpha = 0.6, level = 95) +
  ggtitle("NSA: Train Set")+ 
  theme(legend.position = "none")


arima.nsa.forecast <- NSA_models %>%
  forecast(h=12*3-5)

testSA <- co2_present_test %>%
  autoplot(average) +
  autolayer(arima.nsa.forecast, alpha = 0.4, level = 95) +
  ggtitle("NSA: Test Set")


trainNSA | testSA

modelNSA.forecast <- forecast(NSA_models, h = 31)

knitr::kable(fabletools::accuracy(modelNSA.forecast, co2_present_test))
```

We now train to the seasonally adjusted dataset. Like the NSA data above, we need to transform the data by a second order difference. Since the seasonal adjustment already applies a difference at the seasonal level, we only need to apply a first order difference on this data.

```{r estimates for NSA Model, echo = FALSE, message = FALSE, warning=FALSE, out.width="50%", fig.align = 'center'}
co2_present_trainSA |>
  gg_tsdisplay((difference(season_adjust) ), plot_type='partial', lag_max = 36) + 
  ggtitle("Seasonally Adjusted First Differenced EDA")
```

We notice that the PACF is significant at 2 lags. We also see seasonal spikes. We ran a brute force model to find the best ARIMA equation. The fable package returns ARIMA(0,1,1)(4,0,0)[12] (ArimaSA). Addition.ally, we run a polynomial model with index 2

```{r 5b SA, echo = FALSE, message = FALSE, warning=FALSE, out.width="50%", fig.align='center'}

SA_models <- co2_present_trainSA %>%  
  model(arimaSA = ARIMA(season_adjust ~ 1 + pdq(0,1,1) + PDQ(4,0,0)), 
        polySA2 = TSLM(season_adjust ~ poly(trend(), 2, raw = TRUE) + season()),
        )
#glance(SA_models)

arimaSA <- co2_present_trainSA |>
  model(ARIMA(season_adjust ~ 1 + pdq(0,1,1) + PDQ(4,0,0)))
#report(arimaSA)

arimaSA |>
  gg_tsresiduals()+
  ggtitle("Residual Plots for ARIMA(0,1,1)(4,0,0)[12] w/ drift ")


#augment(NSA_models) |>
 # filter(.model=='arima1') |>
  #features(.innov, ljung_box, lag = 24, dof = 5)
```

The ARIMA(0,1,1)(4,0,0)[12] (ArimaSA) produces largely a normal distribution of errors and no significant autocorrelations. We believe the errors have constant variance. The ljung-box test is not significant, with a p value of .758 at 5 dof and 24 lags, confirming our findings. Thus, we are ready to test our ArimaSA model.

```{r forecast SA 5b, echo = FALSE, message = FALSE,warning = FALSE, fig.width=10, fig.height=5, fig.align='center'}
fitted_SA <- SA_models %>% fitted()

trainSA <- co2_present_trainSA %>%
  autoplot(season_adjust) +
  autolayer(fitted_SA, alpha = 0.8, level = 95) +
  ggtitle("SA: Train Set") +
  theme(legend.position = "none")

modelSA.forecast <- forecast(SA_models, h = 31)

testSA <- co2_present_testSA %>%
  autoplot(season_adjust) +
  autolayer(modelSA.forecast, alpha = 0.6, level = 95) +
  ggtitle("SA: Test Set")

trainSA | testSA

knitr::kable(fabletools::accuracy(modelSA.forecast, co2_present_testSA))
```

In the training data, the models fit the data well. The polynomial model looks to be a little more smooth. Meanwhile, the Arima model fits the non-linear trends of the model more specifically.

In the test data, the two models perform differently. The ARIMA model underestimates the CO2 concentration. Towards the most recent data, the Arima's 95% predictions do not contain the realized value of CO2 concentration. The degree 2 polynomial, on the other hand, largely overestimates the data. A significant percentage of the polynomial's 95% predictions do not contain the actual value. This indicates that both models have a poor bias.

We can tell visually that the ArimaSA model is a better fit of the data. This is also clear in the RMSE metric, as ArimaSA has a value of 0.652 and polySA2 has a value of .743.


## (3 points) Task Part 6b: How bad could it get?

```{r task 6b,  echo = FALSE, message = FALSE, fig.width=10, fig.height=5, center=TRUE, fig.align='center'}
finalArima <- co2_present_train |>
  model(ARIMA(average ~ 0 + pdq(1,1,2) +PDQ(1,1,2)))
#report(finalArima)

arima.forecast <- finalArima %>% 
  forecast(h=1250)


hilo <- hilo(arima.forecast, level = 95)
arima.forecast['lower'] <- as.numeric(sub("\\[(.*),.*\\]95", "\\1", hilo$`95%`))
arima.forecast['upper']  <- as.numeric(sub("\\[.*,(.*)\\]95", "\\1", hilo$`95%`))

if (isFALSE(getOption('knitr.in.progress'))) {
  interval_est2 = function(ppm) {
    first = arima.forecast %>% filter(.mean >= ppm) %>% head(1)
    first.lower = arima.forecast %>% filter(lower >= ppm) %>% head(1)
    first.upper = arima.forecast %>% filter(upper >= ppm) %>% head(1)
    
    last = arima.forecast %>% filter(.mean <= ppm) %>% tail(1)
    last.lower = arima.forecast %>% filter(lower <= ppm) %>% tail(1)
    last.upper = arima.forecast %>% filter(upper <= ppm) %>% tail(1)
    
    return(rbind(first, first.lower, first.upper, last, last.lower, last.upper))
  }

  interval_est2(420)
  interval_est2(500)
  
  print("Confidence interval for Jan 2122")
  arima2122 <- arima.forecast %>% filter(year(index) >= 2122)
  arima2122[1,]
}

co2_present_test %>%
  autoplot(average) +
  autolayer(arima.forecast, alpha = 0.6, level = 95)   + 
  ggtitle("PPM Forecast of ARIMA(1) in 5b") +
  geom_hline(yintercept = 420, linetype = "dashed", color = "black") +
  geom_hline(yintercept = 500, linetype = "dashed", color = "black")

```

Here are the forecasts of when the ARIMA model predicts that 420ppm and 500ppm will be reached. The 95% confidence intervals are given in parentheses after each value. The model predicts that 420ppm will happen for the first time in April 2022 (May 2021, May 2022) and for the last time in Oct 2023 (Nov 2023, Nov 2024). The model predicts that 500ppm will happen for the first time in Nov 2050 (May 2049, Apr 2056) and for the last time in Apr 2069 (Sep 2058, Oct 2072).

In January 2122, the point estimate for the CO2 concentration is 651.37ppm. The 95% confidence interval is from 570.38ppm to 732.37ppm. We have low confidence in this prediction being accurate due to the extended period of the forecast. We are forecasting out a longer interval than we have currently have in existing data. Moreover, the confidence interval is extremely wide, indicating extreme uncertainty.

In conclusion, the rate of CO2 increase is alarming. As we have referenced in multiple points of the study, the increase in CO2 has a myriad of negative impacts to our environment. If the levels of CO2 continue to increase at this level, the entire world will need to drastically adapt.


# Appendix

## Appendix 1: Log transform

```{r log transformation, fig.width=10, fig.height=5, echo = FALSE}
co2$value.bc <- BoxCox(co2$value, lambda = bc.lambda)
co2.orig <- co2 %>% ggplot(
  aes(x = index)) + 
  geom_line(aes(y = value)) +
  ylab("CO2 (ppm)") + xlab("Year Month") + 
  ggtitle("CO2")
co2.log <- co2 %>% ggplot(
  aes(x = index)) + 
  #geom_line(aes(y = value.bc)) +
  geom_line(aes(y = log(value))) +
  ylab("Log(CO2) (ppm)") + xlab("Year Month") + 
  ggtitle("Log(CO2)")
co2.orig|co2.log
```

## Appendix 2: Polynomial trend model performance

```{r polynomial model performance, echo = FALSE, message = FALSE}
knitr::kable(poly.eval)
knitr::kable(fabletools::accuracy(poly.forecast, co2))
```

## Appendix 3: Unit root tests

```{r Unit Root Tests, echo = FALSE, message=FALSE, warning=FALSE}
## ADF Tests
adf.test(co2$value, alternative = "stationary")
adf.test(co2$double.diff[14:nrow(co2)], alternative = "stationary")

## Unit Root Tests
co2 %>%
  features(value, unitroot_kpss)
co2 %>%
  features(double.diff, unitroot_kpss)
```

## Appendix 4: ARIMA Models performance comparison

```{r ARIMA models - train/test performance, , echo=FALSE, message=FALSE, warning=FALSE}
fabletools::accuracy(model.forecast, co2) %>% knitr::kable()
```
